---
author: "KOUASSI PAUL EMMANUEL"
title: "CANCER ANALYSE"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
library(reticulate)
```

We will conduct a study on the **cancer** database. Let's start by importing the necessary modules and the database

```{r}
library(tidyverse)
library(recipes)
library(workflows)

data <- read.csv("Cancer_Data.csv")
```

# PART ONE: EXPLANATORY DATA ANALYSIS (EDA)

```{r}
glimpse(data)
```

Our dataset includes 32 columns, 1 of which presents the cancer ID. It will be removed before our analysis. Plus, we can observe the presence of an unknown column in the original database named `Unnamed: 32`. This one will also be removed. we'll change also target vairable type to **factor**

```{r}
data <- data %>% 
  select(-c(X, id)) %>%
  mutate(diagnosis = as.factor(diagnosis))
```

Now we can look at the characteristics of our database

```{r paged.print=FALSE}
data %>% skimr::skim()
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

target = r.data["diagnosis"]
graph = sns.countplot(x=target)
graph.set_title("Cancer distribtion")
graph.bar_label(graph.containers[0], label_type = 'edge')
plt.show()
```

```{r, include=FALSE, echo=FALSE}
# Cancer distribution with ggplot 
diagnosis_count <- data %>% 
  group_by(diagnosis) %>% 
  summarise(count = n())

diagnosis_count %>% 
  ggplot(aes(x = diagnosis, y = count)) +
  geom_bar(stat = "identity", fill = c("steelblue", "red")) +
  ggtitle(label = "Cancer distribution", subtitle = "Malignant and Belignant") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_text(aes(label = count), vjust = 1.6, color = "white",
            position = position_dodge(0.9), size = 3.5) 
```

```{r}
cor_data <- data %>%
  select(-diagnosis) %>%
  cor()
  
ggcorrplot::ggcorrplot(cor_data, hc.order = TRUE,
                       type = "lower")
```

A lot of information is to be drawn from this analysis:

-   **The database does not contain any missing data, nor duplicate data**

-   **The target variable which is categorical is not unbalanced**. This saves us the resampling and its bias by extension

-   **The presence of a strong correlation between some of the explanatory variables**. This would mean that some of them carry the same information.

A model based on variables carrying the same information would bias our estimates. We'll therefore have to select the variables through **PCA**.

```{r}
library(FactoMineR)
pca <- PCA(data, quali.sup = 1, graph = FALSE)
```

Let's determine the number of principal components to keep

```{r}
library(factoextra)
fviz_eig(pca, addlabels = TRUE)
```

```{r}
library(knitr)
kable(get_eigenvalue(pca))
```

The proportion of variance explained decreases slightly from the 4th component. **Therefore, We'll keep 4 main components**.

```{r}
pca <- FactoMineR::PCA(data, quali.sup = 1, graph = FALSE, ncp = 4)
```

The correlation circle. We will display the names of the variables best represented by the principal component

```{r}
plot.PCA(pca, choix = 'var', habillage = 'cos2', select = 'cos2  0.7',
         unselect = 0, cex = 0.55, cex.main = 0.55, cex.axis = 0.55,
         title = "Correlation circle of variable", axes = 1:2)
```

```{r}
plot.PCA(pca, choix = 'var', habillage = 'cos2', select = 'cos2  0.7',
         unselect = 0, cex = 0.55, cex.main = 0.55, cex.axis = 0.55,
         title = "Correlation circle of variable",axes = 3:4)
```

The quality of the representation of the variables

```{r}
fviz_cos2(pca, choice = 'var', axes = 1:2)
fviz_cos2(pca, choice = 'var', axes = 3:4)
```

and the contribution to the main axes

```{r}
fviz_contrib(pca, choice = "var", axes = 1:2, top = 30)
fviz_contrib(pca, choice = "var", axes = 3:4, top = 30)
```

The variables that are best represented by the principal axes are:

-   **area_mean**

-   **radius_mean**

-   **area_worst**

-   **radius_worst**

-   **perimeter_mean**

-   **perimeter_worst**

-   **concave.point_mean**

-   **concave.point_worst**

-   **concavity_mean**

-   **concavity_worst**

-   **compactness_mean**

-   **compactness_worst**

-   **fractal_dimension_mean**

-   **texture_mean**

-   **texture_worst**

-   **texture_se**

Plus, there is a strong positive correlation between :

-   **area_mean** and **radius_mean,**

-   **area_worst** and **radius_worst,**

-   **perimeter_mean** and **and perimeter_worst**

-   **concave.points_mean** and **concave.points_worst**

-   **concavity_mean** and **concavity_worst**

-   **compactness_mean** and **compactness_worst**

-   **texture_mean** and **texture_worst**

In this case, we will retain among these correlated variables, those which have a better contribution, namely:

-   **area_mean**

-   **perimeter_worst**

-   **concave.points_mean**

-   **concavity_mean**

-   **compactness_mean**

-   **fractal_dimension_mean**

-   **texture_worst**

-   **texture_se**

# PART 2: MODELING

Let's start by splitting our dataset into train and test

```{r}
library(rsample)
set.seed(123)

train_test_split <- initial_split(data)
train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```

We create recipe to pre-processing

```{r}
library(recipes)
data_rc <- recipe(diagnosis ~ ., data = train_data) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 4) %>%
  prep()
```

Then, we create the model

```{r}
library(parsnip)
```

## Logistic Regression

```{r}
# Create model
lr <- logistic_reg() %>% set_engine('glm') %>% translate()
lr
```

**Note:** Use `translate()` to show the model fit template.

```{r}
# Create workflow
library(workflows)
lr_workflow <- workflow() %>%
  add_recipe(data_rc) %>%
  add_model(lr)
```

Note: **workflows** packages is used to **bundle** together the **pre-processing, modeling and post-processing requests**

### Fit the model

```{r}
# fit the model
lr_fit <- fit(lr_workflow, data = train_data)
```

```{r}
# Estimate coefficient
kable(lr_fit %>% extract_fit_parsnip() %>% tidy())
```

All coefficients of the model are significant (p_value \< 0)

### Predict train

```{r}
# Show the first 10 rows
lr_aug <- augment(lr_fit, train_data)
kable(lr_aug %>% select(diagnosis, .pred_class, .pred_M) %>% slice(1:10))
```

#### Confusion matrix

```{r}
library(yardstick)
lr_cm <- lr_aug %>% conf_mat(truth = diagnosis, estimate = .pred_class)
lr_cm %>% autoplot(type = "heatmap")
```

```{r}
# classification report
multi_metric <- metric_set(accuracy, kap, recall)
kable(lr_aug %>% multi_metric(truth = diagnosis, estimate = .pred_class))
```

#### Roc curve

```{r}
lr_aug %>% roc_curve(truth = diagnosis, .pred_B) %>% autoplot()
```

```{r}
# AUC value
kable(lr_aug %>% roc_auc(truth = diagnosis, .pred_B))
```

### Predict test

```{r}
# Show the first 10 rows
lr_aug_test <- augment(lr_fit, test_data)
kable(lr_aug_test %>% select(diagnosis, .pred_class, .pred_M) %>% slice(1:10))
```

#### Confusion matrix

```{r}
lr_cm <- lr_aug_test %>% conf_mat(truth = diagnosis, estimate = .pred_class)
lr_cm %>% autoplot(type = "heatmap")
```

```{r}
# classification report
kable(lr_aug_test %>% multi_metric(truth = diagnosis, estimate = .pred_class))
```

#### Roc curve

```{r}
lr_aug_test %>% roc_curve(truth = diagnosis, .pred_B) %>% autoplot()
```

```{r}
# AUC Value
kable(lr_aug_test %>% roc_auc(truth = diagnosis, .pred_B))
```

## Random Forest

The random forest model is a model that is particularly robust even when the variables are not on the same scale or are correlated. This is the reason why we will use the native database to realize this model

```{r}
# Create the model
rf <- rand_forest(mtry = sqrt(ncol(train_data)), min_n = tune(), trees = tune()) %>%
  set_engine('randomForest') %>%
  set_mode('classification') %>%
  translate()

rf
```

**Note:** for a classification problem, it's shown that the theorical approximate value for the `mtry` parameter is $\displaystyle{\sqrt{p}}$ where $p$ is the number of variable.

### Tune parameters

First, we'll create the grid value for parameter

```{r}
set.seed(123)
library(dials)
library(tune)
rf_grid <- grid_regular(
  parameters(rf),
  levels = 8)
rf_grid
```

Next, the cross-validation folds

```{r}
rf_folds <- vfold_cv(train_data)
rf_folds
```

We can create the workflow and apply the grid search

```{r}
rf_workflow <- workflow() %>%
  add_model(rf) %>%
  add_formula(diagnosis ~ .)

rf_tune <- rf_workflow %>% 
  tune_grid(
    resamples = rf_folds,
    grid = rf_grid
  )
```

and select the best parameters

```{r}
# Show the 5 best parameters
kable(rf_tune %>% show_best("roc_auc"))
```

```{r}
rf_best <- rf_tune %>% select_best("roc_auc")
rf_best
```

We can now finalize our model with these hyper-parameters

```{r}
rf_final <- rf_workflow %>% finalize_workflow(rf_best)
rf_final
```

### Fit the model

```{r}
# fit the model
rf_fit <- fit(rf_final, data = train_data)
```

### Predict train

```{r}
# Show the first 10 rows
rf_aug <- augment(rf_fit, train_data)
kable(rf_aug %>% select(diagnosis, .pred_class, .pred_M) %>% slice(1:10))
```

#### Confusion matrix

```{r}
rf_cm <- rf_aug %>% conf_mat(truth = diagnosis, estimate = .pred_class)
rf_cm %>% autoplot(type = "heatmap")
```

```{r}
# classification report
kable(rf_aug %>% multi_metric(truth = diagnosis, estimate = .pred_class))
```

#### Roc curve

```{r}
rf_aug %>% roc_curve(truth = diagnosis, .pred_B) %>% autoplot()
```

```{r}
# AUC value
kable(lr_aug %>% roc_auc(truth = diagnosis, .pred_B))
```

### Predict test

```{r}
# Show the first 10 rows
rf_aug_test <- augment(rf_fit, test_data)
kable(rf_aug_test %>% select(diagnosis, .pred_class, .pred_M) %>% slice(1:10))
```

#### Confusion matrix

```{r}
rf_cm <- rf_aug_test %>% conf_mat(truth = diagnosis, estimate = .pred_class)
rf_cm %>% autoplot(type = "heatmap")
```

```{r}
# classification report
kable(rf_aug_test %>% multi_metric(truth = diagnosis, estimate = .pred_class))
```

#### Roc curve

```{r}
rf_aug_test %>% roc_curve(truth = diagnosis, .pred_B) %>% autoplot()
```

```{r}
# AUC Value
kable(rf_aug_test %>% roc_auc(truth = diagnosis, .pred_B))
```

The log linear regression model has better accuracy and AUC than the random forest model. This is probably due to the fact that we used the **original database** to build the random forest model while we used the **transformed dataset** for the logistic regression model. This does not prevent the random forest model from providing excellent results despite the presence of a strong correlation between some predictor variables.

This confirms that the **random forest model is robust**
